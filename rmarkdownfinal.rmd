---
title: "Data Mining Final Project"
author: "Karlo Vlahek, Kashaf Oneeb, John Walkington"
date: "5/8/2022"
output: md_document
always_allow_html: true
---

# Spotify: Telling Stories of Music Genre Through Data

```{r, include = FALSE}
library(tidyverse)
library(LICORS)  # for kmeans++
library(foreach)
library(mosaic)
library(vctrs)
library(tidyr)
library(arules)
library(arulesViz)
library(igraph)
library(foreach)
library(knitr)
library(kableExtra)
library(ggplot2)
library(ggpubr)
library(factoextra)
library(ggcorrplot)
library(formattable)
library(randomForest)
library(rpart)
library(rpart.plot)
library(xgboost)
library(ggfortify)
library(patchwork)
library(rsample)
library(gt)
library(gtExtras)
library(gbm)
library(pdp)
library(text2map)
library(naivebayes)
library(pdfCluster)
set.seed(1234)
spotify = read.csv("~/Desktop/final/sampled.csv")
spotify = na.exclude(spotify) 
spotify[sapply(spotify, is.character)] = lapply(spotify[sapply(spotify, is.character)],
                                                           as.factor)
```
## Introduction

Spotify is one of the largest audio and media streaming service providers. Thus, the data collection for this company is vast. This allows for rich data sets that helps Spotify learn about their customers. This project takes a Spotify data set from the SpotifyR package. SpotifyR is an R wrapper which pulls track audio features and other information from Spotify's Web API. We attempt to tell a story through both supervised and unsupervised methodologies. In doing so, the goal is to [expand on this, won't be clear until you go through stuff]

## Data 

The data features are plentiful but ambiguous at first glance. Let's go through each feature and describe what's happening under the hood. Spotify has curated these unique features to help them assess information about the songs they recommend to listeners.


### Features of the Data set (Categorical)

These features are relatively less ambiguous than the qualitative ones. Either way, we describe them in the following sub-section.

**Track_id** is the unique song ID.

**Track_name** is the title of the track.

**Track_artist** is the artist of the track.

**Track_album_id** is the unique id of the album the track is located in.

**Track_album_name** is the name of the album the track is located in.

**Track_album_release_date** is the release date of the album the track is located in.

**Playlist_name** is the name of the playlist the track is located in.

**PLaylist_id** is the unique id of the playlist.

**Playlist_Genre** is the genre of the playlist.

**Playlist_subgenre** is the sub-genre of the playlist.


### Features of the Data set (Quantitative)

**Track_Popularity** is a track's popularity ranging from 0 to 100. The higher the score, the higher the track's popularity. According to Spotify, popularity is calculated by an algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past.

**Danceability** describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least 'danceable' and 1.0 is most danceable.

**Energy** represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, Slayer, a death metal band, surely has high energy. Meanwhile a track composed by Mozart may score low on the scale. Features contributing to this attribute include general entropy, perceived loudness, dynamic range, timbre, and onset rate.

**Key** ranges from integers 0 to 11 and map to pitches that represent the key a track is in. This is because when analyzing post-tonal music, and assuming octave and enharmonic equivalence is appropriate (this is a fancy way of saying we classify what sounds sound the same), integers can represent pitch class. For example, all C’s and any notes that are enharmonically-equivalent to C (like B-Sharp) are pitch class 0. All C-sharps’s and any notes that are enharmonically-equivalent to C-sharp (like D-flat) are pitch class 1. The full table of pitch classes mapped to integers are below.

```{r, include = FALSE}
integers = c(0,1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)
pitches = c("C", "C-Sharp", "D", "D-Sharp", "E", "F", "F-Sharp", "G", "G-Sharp", "A", "B-Flat", "B")
key_table = data.frame(integers,pitches)
```

```{r,echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
knitr::kable(key_table, col.names = c("Integer", "Pitch"),
                               caption = "Key Mappings",
                               booktabs = TRUE) %>% 
  kable_styling(position = "center")
```

**Loudness** is the quality of a sound. It is the analog of a physical stength or amplitude of a track. It is measured in decibels (dB) and averaged across the entire track. Loudness is useful for comparing relative loudness among songs in the data set. It ranges roughly from -60 to 0.5 dB.

**Mode** indicates the modality of a track and thus the type of scale from which the tracks melodic content is derived from. It is a binary variable that assigns a 0 to tracks that have a minor scale and a 1 to tracks that have a major scale.

**Speechiness** detects the presence of spoken words in a track. If a track appears more speech-like in its recording (like a podcast might), then the speechiness score will be greater. This is a proportion, and thus the features values range from 0 to 1. For this feature, values that are less than 0.33 are considered music and perhaps even instrumental music. Values between 0.33 and 0.66 are generally considered music tracks that include both music and speech. Podcast such as tracks have a proportion of 0.66 or greater. Since this data set only has music, we will see the former range throughout the data set. Furthermore, features such as speechiness are exactly why scaling the data is necessary. Allowing for raw ranges of values would create a bias in the data results.

**Acousticness** represents a confidence measure from 0 to 1 on whether the track is acoustic. 1.0 represents a high confidence and 0 represents a low confidence.


**Instrumentalness** represents a prediction of whether the track contains vocals and the scale is from 0 to 1. The greater the score, the greater the likelihood the track is to instrumental. Tracks that have more vocals (such as rap) score lower on this scale. Tracks that score 0.5 or higher are *intended* to represent instrumental tracks. Thus, scaling the data provides useful for how tracks compare in this data set.

**Liveness** detects the presence of an audience in the recorded track. This could be, for example, Pink Floyd's live performance of their Dark Side of the Moon album. There is clearly a difference between this recording and the album when it was recorded in a studio setting. Typically, if a track scored higher than 0.8 it is likely it was recorded with an audience. Again, scaling will provide useful for this feature.

**Valence** measures the musical *positiveness* of a track and is scored from 0 to 1. Tracks with valence closer to 1 sound more positive (i.e. - cheerful, euphoric) and tracks with low valence appear more negative (i.e. - sad, angry).

**Tempo** is an overall estimate of a track's beats per minute (BPM). This is the speed of a track that is calculated by taking the average beat duration.

**Duration_ms** is simply the duration of the track in milliseconds.


There are two issues that arise with the data. The first issue was already alluded to which is the measurements of this data is over all the tracks that Spotify offers to listeners. Thus, this data will be sampled and scaled before performing any statistical modeling. The second issue is that many of these features seem a bit repetitive in measuring similar attributes of tracks. For example, what is the difference between **Acousticness**, **Speechiness**, and **Instrumentalness**? The answer is that the latter two features represent a prediction and probability *score* while the former feature actually *detects* what is within a track. Regardless, this will serve as great reason to perform a PCA analysis to understand groups of data. Performing clustering methods will provide meaningful, interpretable information. However, the caveat is that clustering methods partition data (in this case, the tracks) into mutually exclusive groupings. Given the data, this method has shortcomings that must be taken into consideration. However, we still cluster to survey various methods. 

## Data Exploration

The following section allows for exploration of the data to provide some information about relationships among features in the data set. Before diving into methodologies, visualizing relationships provides general but important information to further develop the Spotify genre story while partially motivating what methodologies to choose from.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
spotify %>% 
  count(playlist_genre) %>% 
  knitr::kable(col.names = c("Genre", "Count"), booktabs=TRUE) %>% kable_styling(position = "center", full_width = FALSE)
```


### Popular Artists
```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
artist_counts <- spotify %>%
  group_by(track_artist, playlist_genre) %>%
  summarize(count = n()) %>%
  arrange(desc(count))

head(artist_counts, 20) %>%
  ggplot() +
  geom_col(aes(x=reorder(track_artist, count), y=count, color=playlist_genre )) + 
  coord_flip() +
  labs(title='Top 20 Artists in each Playlist and their Classified Genre',
       x='Artist Name',
       y='# of Artist Tracks in each Playlist'
       ) +
scale_fill_gradient2(
    low = "grey", 
    mid = "white", 
    high = "#00B159", 
    midpoint = .07
  )
```

Above we observe the top artists that appear across all playlists regardless of genre. We see that all artists are classified in one type f genre except for David Guetta who appears 35 times in both pop and EDM playlists This puts David Guetta in third place for this data set. Rock is the most popular genre thanks to Queen and Guns N' Roses.

### Song Popularity
```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

popular_songs <- spotify %>%
  group_by(track_name, track_popularity) %>%
  filter(track_popularity>=95) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
  # arrange(desc(track_popularity))

popular_songs %>%
  ggplot() +
  geom_col(aes(x=reorder(track_name, track_popularity), y=track_popularity), fill='#00B159') +
  labs(title='Songs with Popularity score greater than 95',
       y='Song Popularity on a scale of 0-100',
       x='Song Name'
       ) +
  coord_cartesian(ylim = c(90, 100)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))



```

When filtering for most popular songs in the data set, we observe Roxanne (Arizona Zerva) rated second highest following Dance Monkey (Tones and I), and a 5-way tie with Blinding Lights (The Weekend), Cirlces (Post Malone), Memories (Maroon 5), The Box (Roddy Rich), and Tusa (Carol G).

### Genre Popularity
```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

popular_genres <- spotify %>%
  group_by(playlist_genre) %>%
  filter(track_popularity>=80) %>%
  summarize(avg_popularity= mean(track_popularity), count= n()) %>%
  arrange(desc(count))

popular_genres %>%
  ggplot() +
  geom_col(aes(x=reorder(playlist_genre, avg_popularity), y=avg_popularity), fill='#00B159') +
  labs(title='Genres with Average Popularity Greater than 80',
       y='Song Popularity Average on a scale of 0-100',
       x='Genre'
       ) +
  coord_cartesian(ylim = c(75, 90))

```

Taking the average of song popularity in each playlist genre, we see that EDM has the highest average. With David Guetta and Martin Garrix being two of the artists that appear in the most amount of playlists, this makes sense.

### Popular Subgenres
```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
popular_genres <- spotify %>%
  group_by(playlist_genre,playlist_subgenre) %>%
  filter(track_popularity>=80) %>%
  summarize(avg_popularity= mean(track_popularity), count= n()) %>%
  arrange(desc(count))

popular_genres %>%
  ggplot() +
  geom_col(aes(x=reorder(playlist_subgenre, avg_popularity), y=avg_popularity, fill = playlist_genre)) +
  labs(title='Subgenres with Popularity greater than 80',
       y='Song Popularity on a scale of 0-100',
       x='Subgenre'
       ) +
  coord_cartesian(ylim = c(75, 90)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

The most popular subgenres come from a mix of all 6 major genres. As we have observed, EDM is most popular. However, it does not appear in the most amount of popular subgenres in this data set.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
spotify_features = names(spotify)[12:23]

feature_density = spotify %>%
  select(c('playlist_genre', spotify_features)) %>%
  pivot_longer(cols = spotify_features) %>%
  ggplot(aes(x = value)) +
  geom_density(aes(color = playlist_genre), alpha = 0.5) +
  facet_wrap(~name, ncol = 3, scales = 'free') +
  labs(title = 'Spotify Genre Feature Density',
       x = '', y = 'density', 
       color = "Genres") +
  theme(axis.text.y = element_blank())

feature_density
```

Creating kernel density estimates show the probability distribution functions of each feature, and is essentially a smoothed over histogram. For each feature, there represents a distribution faceted by the genre. At the aggregate level, songs from the data set seem to have low confidence in acousticness, low probability they are instrumental, not likely to have been recorded with a live audience, and low speechiness. However, danceability, valence, loudness, and energy have higher levels of probability associated with them. For separating genres, we will attempt to utilize the three aforementioned features.

## Modeling
Next we incorporate several methodologies to attempt to classify the songs in the data set. We attempt tree models and then attempt to better explain it using PCA. Then, we compare all of them to see what story is told about the genre data set.

### Unsupervised Learning

We decided to implement k-means++ and ward agglomerative clustering to see if track attributes, such as, danceability, energy, loudness, etc. could be useful in predicting track genres. We chose k-means++ as a baseline to compare any succeeding clustering methods against. Moreover, we implemented agglomerative hierarchical clustering hoping that it would capture the hierarchy present in genres and subgenres, if any. The details of the two methods are discussed below.

#### K-means++ clustering

###### Choose k

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# center and scale the attribute columns
X <- spotify[, (12:23)]
X <- scale(X, center=TRUE, scale=TRUE)

# extract the centers and scales from the rescaled data 
mu <- attr(X,'scaled:center')
sigma <- attr(X,'scaled:scale')

# choose k
fviz_nbclust(X, kmeans, method = 'wss')
```

In order to choose k, we utilized the "Elbow Plot" of Total Within Sum of Squares against values of k from 2 to 10. It can be seen that the "elbow" shape is observed around k=5 and decreases consistently beyond that point. Therefore, we decided to test values of k from 5 through 8 and choose the value with the highest Adjusted Rand Index. The Adjusted Rand Index evaluates whether dimension-reduced similarity cluster results are similar to one other. The Rand Index ranges from 0 to 1, where it equals to 0 when points are assigned into clusters randomly and it equals to 1 when the two cluster results are same.

###### Adjusted Rand Index (ARI) for optimal k

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# using kmeans++ initialization

clust5 <- kmeanspp(X, k=5, nstart=25)

clust6 <- kmeanspp(X, k=6, nstart=25)

clust7 <- kmeanspp(X, k=7, nstart=25)

clust8 <- kmeanspp(X, k=8, nstart=25)


actual_genre <- as.numeric(spotify$playlist_genre)

# adjusted rand index:comparison of kmeans with original genres
print('Adjusted Rand Index for genre')
cat('Rand Index for k=5:', adj.rand.index(actual_genre,clust5$cluster), '\n')
cat('Rand Index for k=6:', adj.rand.index(actual_genre,clust6$cluster), '\n')
cat('Rand Index for k=7:', adj.rand.index(actual_genre,clust7$cluster), '\n')
cat('Rand Index for k=8:', adj.rand.index(actual_genre,clust8$cluster))

```

The highest Adjusted Rand Index is observed for k=6. However, the ARI itself is not high enough to represent good clustering, specifically it is approximately 0.06, which means that we had poor recovery for the original genre clusters. Nevertheless, we will use k=6 to display the results for k-means++.

#### K-means++ Cluster Summary

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# display cluster centers k=6 since it has the highest adjusted rand index

as.data.frame(clust6$center[1,]*sigma + mu) %>%
  round(2)
as.data.frame(clust6$center[2,]*sigma + mu) %>%
  round(2)
as.data.frame(clust6$center[3,]*sigma + mu) %>%
  round(2)
as.data.frame(clust6$center[4,]*sigma + mu) %>%
  round(2)
as.data.frame(clust6$center[5,]*sigma + mu) %>%
  round(2)
as.data.frame(clust6$center[6,]*sigma + mu) %>%
  round(2)

```

Above we can see the average scores assigned to each attribute in each of the 6 clusters created by the k-means++ technique. The first cluster appears to have very a low loudness score and a very high key score with moderate energy and instrumentalness. The second cluster appears to be relatively the same except with lower loudness, and instrumentalness, and higher acousticness. Observing the following clusters, it is evident that all of them have negative loudness scores with high magnitudes, as well as, high key scores. It is unfortunate that all the clusters share such similar properties. This explains the low Adjusted Rand Index.

###### Total Within and Between Sum of Squares

The Total Within Sum of Squares and Between Sum of Squares is displayed below. It is clear that the Total Within Sum of Squares is much higher than the Between Sum of Squares, which is not ideal as typically more compact clusters have lower Total Within Sum of Squares and higher Between Sum of Squares.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# calculate within and between ss for k=6

cat('Total Within Sum of Squares:',clust6$tot.withinss, '\n') 
cat('Total Between Sum of Squares:',clust6$betweenss, '\n') 

```

#### Clustering Visualization

Several clustering comparison plots are displayed below of various attributes plotted against each other. The coloring of the data points is defined by genres. The plots on the left reflect actual clustering observed in the data, whereas the plots on the right reflect k-means++ clustering. It can be seen that there are not distinct genre clusters in the original data to begin with. Perhaps that is why kmeans++ performed so poorly in recovering these clusters. Overall, neither of the plots reflects comparable clustering.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
# Comparing danceability and energy how it relates to genres 
a1=qplot(danceability, energy, data = spotify, color = factor(clust6$cluster), alpha = 0.8)
a2=qplot(danceability, energy, data = spotify, color = playlist_genre, alpha = 0.8)
a2+a1

# Comparing danceability and speechiness how it relates to genres 
b1=qplot(danceability, speechiness, data = spotify, color = factor(clust6$cluster), alpha = 0.8)
b2=qplot(danceability, speechiness, data = spotify, color = playlist_genre, alpha = 0.8)
b2+b1

# Comparing danceability and acousticness how it relates to genres 
c1=qplot(danceability, acousticness, data = spotify, color = factor(clust6$cluster), alpha = 0.8)
c2=qplot(danceability, acousticness, data = spotify, color = playlist_genre, alpha = 0.8)
c2+c1

# Comparing loudness and energy how it relates to genres 
d1=qplot(loudness, energy , data = spotify, color = factor(clust6$cluster), alpha = 0.8)
d2=qplot(loudness, energy, data = spotify, color = playlist_genre, alpha = 0.8)
d2+d1

# Comparing instrumentalness and acousticness how it relates to genres 
e1=qplot(instrumentalness, acousticness, data = spotify, color = factor(clust6$cluster), alpha = 0.8)
e2=qplot(instrumentalness, acousticness, data = spotify, color = playlist_genre, alpha = 0.8)
e2+e1

# Comparing liveness and valence how it relates to genres 
f1=qplot(liveness, valence, data = spotify, color = factor(clust6$cluster), alpha = 0.8)
f2=qplot(liveness, valence, data = spotify, color = playlist_genre, alpha = 0.8)
f2+f1

# Comparing instrumentalness and speechiness how it relates to genres 
f1=qplot(instrumentalness, speechiness, data = spotify, color = factor(clust6$cluster), alpha = 0.8)
f2=qplot(instrumentalness, speechiness, data = spotify, color = playlist_genre, alpha = 0.8)
f2+f1
```

#### Agglomerative Hierarchical Clustering

The second clustering approach implemented in the analysis is agglomerative hierarchical clustering. In order to decide which linkage method would be ideal, we decided to observe the balance in terms of cluster distrbution i.e. the number of observations in each cluster. We tested the complete, ward, single, and average linkage methods for k=6 (chosen from the "Elbow Plot" below). The cluster distribution for each method is shown below. It can be seen that the most balanced and evenly distbution observations arise from the ward method, making it the method of choice. For the ward method, the linkage function specifying the distance between two clusters is computed as the increase in the "error sum of squares" after fusing two clusters into a single cluster.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# choose k to cut tree
fviz_nbclust(X, FUN = hcut, method = "wss")
```

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# agglomertive hierarchical clustering 
# form a pairwise distance matrix using the dist function
distance_matrix = dist(X, method='euclidean')

# run agglomerative clustering for complete, single, average
hier_complete <- hclust(distance_matrix, method='complete')
hier_ward <- hclust(distance_matrix, method='ward')
hier_single <- hclust(distance_matrix, method='single')
hier_average <- hclust(distance_matrix, method='average')

# test balance for k=6 for each method since there are 6 genres
complete <- cutree(hier_complete, k=6)
ward <- cutree(hier_ward, k=6)
single <- cutree(hier_single, k=6)
average <- cutree(hier_average, k=6)

cat('Cluster balance for method=complete', '\n')
summary(factor(complete))

cat('Cluster balance for method=ward', '\n')
summary(factor(ward))

cat('Cluster balance for method=single', '\n')
summary(factor(single))

cat('Cluster balance for method=average', '\n')
summary(factor(average))

```

#### Ward Dendrogram 

The dendrogram for the ward clustering is displayed below with different colored rectangles encapsulating the clusters formed. The dendrogram is relatively balanced, however the inital split on the right seems to divide further a lot more than the inital split on the left. 

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# since k=6 seems optimal, plot dendrogram for k=6
plot(hier_ward, cex = 0.6)
rect.hclust(hier_ward, k = 6, border = 2:5)

```

#### K-means++ vs. Ward Clustering

The cluster plots for the two clustering methods are displayed below. K-means++ resulted in much more distinct and defined clustering than Ward Clustering, which results in a lot more overlapping of clusters. 

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}

# plot clusters obtained using kmeanspp and ward clustering
g1=fviz_cluster(clust6, X, ellipse.type = "norm")
g2=fviz_cluster(list(data = X, cluster = ward))
g1+g2
```

We compare the Adjusted Rand Index for both the methods against actual genre clusters. K-means++ clusters are more similar to the actual genre clusters than the ward clusters since K-means++ has a higher Adjusted Rand Index. 

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}


# adjusted rand index:comparison of kmeans with ward clustering
print('Adjusted Rand Index for genre')
cat('Rand Index for ward clustering for k=6:', adj.rand.index(actual_genre,ward), '\n')
cat('Rand Index for kmeans++ for k=6:', adj.rand.index(actual_genre,clust6$cluster), '\n')
```

### PCA Analysis
We look at a principle component analysis for dimension reduction techniques to incorporate in our bake-off of supervised learning models. 

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
ggcorrplot::ggcorrplot(cor(X), hc.order =TRUE)
```

When observing correlations of features, we re-order the features according to hierarchical clustering. In the top left and bottom right regions of the hierarchical correlation plot, we can observe that acousticness has a strong, negative correlation with energy and loudness. Perhaps acoustic songs make listeners generally feel less energy, and they are also not associated with tracks that have higher dB (i.e. - loudness). Danceability and valence have a strong, positive correlation. Perhaps this correlation can be attributed to the idea that if songs make listeners feel happier, then they will want to dance more. Some relationships that do not seem intuitive is the negative correlation between danceability and tempo, where higher tempo may induce listeners to dance more.

```{r, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
PCA_Spotify=prcomp(X, scale=TRUE, rank =3)
summary(PCA_Spotify)
```

There are several takeaways from observing the summary of the analysis. We see the standard deviation of the PCs is highest in PC1. The proportion of variance each PC accounts for from the original data is low and hovers between .09 and .1787, but this proportion is highest in PC1. Cumulatively, all three principle components account for a little more than two-thirds of the variation from the original data. Since the variation is not well preserved in the the three principle components, the CLUSTERING METHOD IS PREFERERD OVER THS ONE.

```{r include = FALSE}
knitr::kable(round(PCA_Spotify$rotation[,1:3],2),
             col.names = c("PC1", "PC2","PC3"),
                               caption = "Principle Components",
                               booktabs = TRUE)
```

Looking at the table, we notice similar loadings in PC1 as the correlation plot such as danceability and loudness as well as the opposing magnitudes of danceability and acousticness as well as danceability and loudness. The higher, positive magnitudes associated with each feature represents stronger positive correlations and higher, negative magnitudes associated with each feature represents stronger, negative correlations. PC2 shows danceability and valence moving in a similar direction as well.

```{r echo = FALSE}

autoplot(PCA_Spotify, data = spotify, colour = 'playlist_genre', alpha = 0.6, 
         loadings = TRUE, loadings.colour = 'pink',
         loadings.label = TRUE, loadings.label.size = 3)
```

When plotting the first two components against one another, we see that the PCA does not explain the variability well at all. The variability is seldom captured by the PCA as per the previous PCA summary table. Rock seems to push towards the upper left quadrant which maps slightly better towards PC2 than PC1.

### Decision trees and random forests

One technique we have not utilized thus far for classification is decision trees and random forests.  Decision trees are a great method for classification because they are nonparametric and tend to work well out of the box.  However, using a singular decision tree makes it prone to overfitting and capturing random noise in the data.  To fix this we can use random forests, which aggregate predictions over a large number of decision trees.  Let's first try a singular tree for playlist genre classification:

```{r echo = FALSE}
spotify_split = initial_split(spotify, prop = 0.8)
spotify_train = training(spotify_split)
spotify_test = testing(spotify_split)

tree1 = rpart(playlist_genre ~ danceability + energy + key + loudness + mode + speechiness + acousticness + instrumentalness + liveness + valence + tempo + duration_ms, data = spotify_train)

rpart.plot(tree1)
```

Here in our dendrogram we can see several decision splits that make sense.  The first split it makes is based on speechiness, the density of spoken words in a song.  Right off the bat, it calls everything with speechiness >= 0.18 rap, which makes sense intuitively.  Rap songs tend to have the most spoken content.  It then classifies everything that has danceability < 0.55 as rock, which also makes sense--you can't really dance to Metallica like you would Latin or EDM.

One problem we can see right off the bat, though, is that our decision tree does not even classify anything as Pop, even though our training data has 1344 Pop playlist songs.  This is a significant classification error by our model, so it's something we can improve on.  Let's look at our out-of-sample classification accuracy:

```{r echo = FALSE}
y_hat_test = predict(tree1, newdata = spotify_test, type = "class")

table(y_hat_test, spotify_test$playlist_genre)
sum(diag(table(y_hat_test, spotify_test$playlist_genre)))/2000
```

Here we have a table of our predicted class vs. our true class, and a classification accuracy of 40.4 percent.  This is not great, so we'll now try a random forest to see if our predictions get better.  We can also draw a lot of valuable insights from variable importance plots with random forests.

```{r include = FALSE}
forest1 = randomForest(playlist_genre ~ danceability + energy + key + loudness + mode + speechiness + acousticness + instrumentalness + liveness + valence + tempo + duration_ms, data = spotify_train, importance = TRUE)
```

After running a random forest of our playlist genre on our key Spotify-developed music metrics (danceability, acousticness, instrumentalness, etc.), we can make a variable importance plot to tell us which features are most crucial for correct classification of genre:

```{r echo = FALSE}
varImpPlot(forest1, type = "1")
```

This tells us that speechiness and danceability are by far the most important variables for accurate classfication--to omit these variables would each lead to more than a 100 percent decrease in classification accuracy.  All our variables are useful, here though, so we can include them all in our forest.  We can also see how many trees it takes for our forest to provide the most accurate classifications:

```{r echo = FALSE}
plot(forest1)
```

Here we can see that overall classification error bottoms our at about 325 trees, but some genres (denoted by different colored lines on the plot) are still much more harder to classify than others.  The blue line at the top is our notoriously-difficult-to-classify Pop genre.

Let's look at overall classification accuracy with our new random forest:

```{r echo = FALSE}
y_hat_test = predict(forest1, newdata = spotify_test, type = "class")

table(y_hat_test, spotify_test$playlist_genre)
sum(diag(table(y_hat_test, spotify_test$playlist_genre)))/2000
```

Looks like we're performing a lot better now!  Our model is now correctly identifying some pop songs, and our overall classification accuracy has increased by 13 percent.

Next we will try our final tree-based classification method, gradient-boosted trees.  Gradient-boosted trees use a shrinkage factor to avoid overfitting, but are more sensitive to hyperparameter tuning than random forests.  We'll start out with an interaction depth of 4, 500 trees, and a shrinkage factor of .05.

```{r echo = FALSE}
spotify_boost = gbm(playlist_genre ~ danceability + energy + key + loudness + mode + speechiness + acousticness + instrumentalness + liveness + valence + tempo + duration_ms, data = spotify_train,
                   interaction.depth=4, n.trees=500, shrinkage=.05)

as.data.frame(summary(spotify_boost)) %>%
  gt() %>%
  gt_theme_nytimes() %>%
  tab_header(title = "Variable name and relative influence")
```

Our gradient-boosted tree came up with slightly different results for what influences classification accuracy than our random forest did.  Speechiness and danceability are still on top, but we see now that tempo is the most influential variable in improving classification accuracy.  Let's see if our gradient-boosted tree was more accurate:

```{r echo = FALSE}
y_hat_test = predict(spotify_boost, newdata = spotify_test, type = "response") %>%
  apply(1, which.max)

table(y_hat_test, spotify_test$playlist_genre)
sum(diag(table(y_hat_test, spotify_test$playlist_genre)))/2000
```

Here we get a very tiny improvement in performance for our gradient-boosted tree (.1 percent) over our random forest.  This might tell us something important: that tempo is more informative for classification than we previously thought.

### Predicting with song titles

One of our group members was consumed with the question: do certain genres use certain words more frequently in song names, and can we classify genre with nothing but song and album titles?

The tool we used to answer this question was Multinomial Naive Bayes, which is essentially regression in reverse: it involves predicting features based on a class label.  We started with preprocessing: we took all of the songs in our dataset and engineered a column that combined song and album name, all lowercase and without special characters.  We then fed our list of song names and word strings into the "text2map" document-term-builder tool to build a sparse matrix that one-hot-encoded the presence of certain words in a song and album title.  We then trained our data on a subset of the genre labels and the document-term-matrix with Multinomial Naive Bayes, and then saw how accurately we could predict genres.  Let's see how we did:

```{r echo = FALSE}
naiveprep = spotify %>%
  select(track_name, track_album_name, playlist_genre) %>%
  mutate(combo = paste(track_name, track_album_name)) %>%
  mutate(cleancombo = tolower(gsub("[[:punct:]]", "", combo))) %>%
  select(track_name, playlist_genre, cleancombo)

X_NB = dtm_builder(data = naiveprep, text = "cleancombo", doc_id = "track_name")
y_NB = factor(naiveprep$playlist)

N = length(y_NB)
train_frac = 0.8
train_set = sample.int(N, floor(train_frac*N)) %>% sort
test_set = setdiff(1:N, train_set)

X_train = X_NB[train_set,]
X_test = X_NB[test_set,]

y_train = y_NB[train_set]
y_test = y_NB[test_set]

nb_model = multinomial_naive_bayes(x = X_train, y = y_train)

y_test_pred = predict(nb_model, X_test)

table(y_test, y_test_pred)

# test-set accuracy
sum(diag(table(y_test, y_test_pred)))/length(y_test)

```

Looks like we can predict with 43.7 percent accuracy, which is not too bad, considering we were training only on the presence of certain words in our song and album titles.  One of the shortcomings of this approach, though, is that artists' names are often in song titles, and this provides artificial advantage given that most artists stick only to certain genres.